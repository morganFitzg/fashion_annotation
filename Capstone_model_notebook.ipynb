{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone-model-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOAFk+rEOKAdbaKjoJgLB63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morganFitzg/fashion_annotation/blob/main/Capstone_model_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flatiron Capstone: Fashion Image Annotation\n",
        "\n",
        "Morgan Fitzgerald\n",
        "\n",
        "Instructor: Abhineet Kulkarni"
      ],
      "metadata": {
        "id": "ki_9xSNDcR0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding"
      ],
      "metadata": {
        "id": "N_Zk4hDScdSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project builds a proof of concept for a data pipeline that produces Instagram trend reports using information from the post images.\n",
        "\n",
        "While hashtag and comment data is readily available, one cannot always ascertain the kind of clothing being pictured from this information.  When making marketing or product development decisions, trend data that includes more specific details on what clothing is being pictured could be very useful. Additionally, fashion influencers, journalists, and stylists could benefit from more detailed trend data.\n",
        "\n",
        "Ultimately, the end user can view the data/results in a google data studio report built from a MySQL Google Cloud database.  The report can be filtered to show different time frames and data from specific categories and also provides a view of the images with the most likes from the selected clothing category."
      ],
      "metadata": {
        "id": "vp6qnbI_chHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding"
      ],
      "metadata": {
        "id": "33dkewUjhKHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used to train this model comes from the [iMaterialist fashion attribute dataset](https://arxiv.org/abs/1906.05750).  The dataset contains over a million images annotated with 8 different attributes.  Our model predicts three of these attributes: category, color and pattern.\n",
        "\n",
        "Upon examination of the data (in this notebook), it was clear that there is a significant amount of mislabeled data and certain labels could easily be confounded.  For example there are separate labels for 'jeans' and 'skinny jeans', yet images of skinny jeans being labeled as 'jeans'. Care was taken to simplify some of these confusing categories and group similar categories where a distinction is not critical for trend analysis.  \n",
        "\n",
        "After cleaning the data, the clothing category attribute has 57 different categories, pattern has 23 and color has 15.  \n",
        "\n",
        "With such a large number of images, it was not possible in the timeframe allocated for this project to correct mislabeling, but to increase accuracy of the model it would be important increase the quality of the data."
      ],
      "metadata": {
        "id": "jojuHyD2hNwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "yE33wZ2_jGZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performing annotations was built using transfer learning with the VGG16 image classification model as a backbone with three additional dense layers added with three branches, one for each attribute. The weights in the VGG16 layers were frozen during training.  Due to resource and time constraints, the model was trained on approximately 100,000 of the million images available. Training was performed in two rounds, two epochs each for two sets of approximately 50,000 images.\n",
        "\n",
        "The overall performance for the model is not at the level that would be required to deploy for commercial use, but it is far better than random guessing.  Below are the validation accuracy scores for each attribute:\n",
        "\n",
        "<ol>\n",
        "<li>Category: 48%</li>\n",
        "<li>Pattern: 75%</li>\n",
        "<li>Color: 53%</li>\n",
        "</ol>\n",
        "\n",
        "There are a number of ways that accuracy could be increased given more time and resources for the project. A few are listed below:\n",
        "\n",
        "<ol>\n",
        "<li>Improve data labeling</li>\n",
        "<li>Train the model on more images</li>\n",
        "<li>Unfreeze the weights in VGG16 layers during training to fine tune</li>\n",
        "<li>Add dense layers</li>\n",
        "<li>Investigate categories that are frequently mislabeled</li>\n",
        "<li>Try different loss function/optimizer</li>\n",
        "</ol>\n",
        "\n",
        "Ultimately, this project is more about a proof of concept and with 59, 23, and 15 categories respectively within each attribute the model is clearly achieving much higher accuracy than random guessing.  Research is ongoing in the field of image and especially fashion image annotation, as this is a challenging problem and a commercial grade model would require more time, resources, and perhaps some adjustments to the architecture.\n",
        "\n",
        "Finally, there are limitations to this model that could be addressed with a more complex architecture. When finding attributes, this model assumes there is only one piece of clothing pictured. However, in many cases there would be multiple items such as a shirt and pants or a dress and jacket. There are neural networks designed to identify different clothing articles within an image (and locate them) allowing different attributes to be assigned to each article. However, this was outside the scope of this project. "
      ],
      "metadata": {
        "id": "kxvPcqUyjjjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "d-kx3kX1E6-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaX8kJBH-_6o",
        "outputId": "1b4303c1-bc80-44e6-a99a-1413a85ca1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "5bcUnJSpKImJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71999bde-aed9-4847-9602-377cb8022189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 17 14:57:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    32W / 250W |    375MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import io \n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "0TuR0vIU_I-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#config\n",
        "IMAGE_SIZE=(224,224,3)\n",
        "epochs=3\n",
        "batch_size=32\n",
        "path='/content/drive/My Drive/fashion_annotation/images/'\n",
        "train_path='/content/drive/My Drive/fashion_annotation/imgs_train1/'\n",
        "val_path='/content/drive/My Drive/fashion_annotation/imgs_val1/'"
      ],
      "metadata": {
        "id": "reFfndvDBrIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UWa37QeCBe0",
        "outputId": "9061da5e-9b65-4e26-9482-a1bf1f80b7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n",
        "\n",
        "The iMaterialist dataset was cleaned and formatted in a separate notebook.\n",
        "\n",
        "In this section, the data is separated into six separate groups stratified by the category attribute (and verified that the other attributes were also properly stratified). This is necessary because the images are downloaded onto google drive which times out when trying to access a file in a folder with too many files.  Therefore, separate subfolders where created for the training and validation sets of each of these 6 splits."
      ],
      "metadata": {
        "id": "IynVlWDaxbcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/drive/My Drive/fashion_annotation/data_v2.csv')"
      ],
      "metadata": {
        "id": "BKvTLu03vGLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vjGTnOzBvKlq",
        "outputId": "f77a28c9-af75-4cba-8cb0-aa96eab688aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   imageId    category  color pattern  \\\n",
              "0        7  Nightgowns  White    None   \n",
              "1        8     Blouses  Black    Mesh   \n",
              "2       11      Skirts  Green    None   \n",
              "3       13     Dresses  Black    None   \n",
              "4       14     Dresses  Black    None   \n",
              "\n",
              "                                                 url  \n",
              "0  http://contestimg.wish.com/api/webimage/52cbee...  \n",
              "1  http://contestimg.wish.com/api/webimage/53994b...  \n",
              "2  http://contestimg.wish.com/api/webimage/51bd84...  \n",
              "3  http://contestimg.wish.com/api/webimage/53e099...  \n",
              "4  http://contestimg.wish.com/api/webimage/543e92...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e870d6ac-299f-41f2-89a0-21dfa16697ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageId</th>\n",
              "      <th>category</th>\n",
              "      <th>color</th>\n",
              "      <th>pattern</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>Nightgowns</td>\n",
              "      <td>White</td>\n",
              "      <td>None</td>\n",
              "      <td>http://contestimg.wish.com/api/webimage/52cbee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>Blouses</td>\n",
              "      <td>Black</td>\n",
              "      <td>Mesh</td>\n",
              "      <td>http://contestimg.wish.com/api/webimage/53994b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>Skirts</td>\n",
              "      <td>Green</td>\n",
              "      <td>None</td>\n",
              "      <td>http://contestimg.wish.com/api/webimage/51bd84...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Black</td>\n",
              "      <td>None</td>\n",
              "      <td>http://contestimg.wish.com/api/webimage/53e099...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Black</td>\n",
              "      <td>None</td>\n",
              "      <td>http://contestimg.wish.com/api/webimage/543e92...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e870d6ac-299f-41f2-89a0-21dfa16697ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e870d6ac-299f-41f2-89a0-21dfa16697ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e870d6ac-299f-41f2-89a0-21dfa16697ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_num=data['category'].nunique()\n",
        "pat_num=data['pattern'].nunique()\n",
        "col_num=data['color'].nunique()\n",
        "print(cat_num)\n",
        "print(pat_num)\n",
        "print(col_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0yGVsP_CUaJ",
        "outputId": "08d22102-c9dc-433a-894a-0bdd3baa9a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57\n",
            "23\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integer codes for each of the attribute's categories are created as required for the keras to_categorical function that is used in the Image Generator."
      ],
      "metadata": {
        "id": "fqQ4TKt26lWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create numerical codes for the category and pattern labels\n",
        "data.category=pd.Categorical(data.category)\n",
        "data['cat_codes']=data.category.cat.codes\n",
        "data.pattern=pd.Categorical(data.pattern)\n",
        "data['pat_codes']=data.pattern.cat.codes\n",
        "data.color=pd.Categorical(data.color)\n",
        "data['col_codes']=data.color.cat.codes\n",
        "\n",
        "#make key for the codes\n",
        "pattern_key=data.groupby('pattern').mean()['pat_codes'].astype(int)\n",
        "category_key=data.groupby('category').mean()['cat_codes'].astype(int)\n",
        "color_key=data.groupby('color').mean()['col_codes'].astype(int)\n",
        "color_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYUAp3JMc7oH",
        "outputId": "6af0c22f-5c1b-48e8-b669-20637fe92607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "color\n",
              "Beige           0\n",
              "Black           1\n",
              "Blue            2\n",
              "Brown           3\n",
              "Gold            4\n",
              "Gray            5\n",
              "Green           6\n",
              "Multi Color     7\n",
              "Orange          8\n",
              "Pink            9\n",
              "Purple         10\n",
              "Red            11\n",
              "Silver         12\n",
              "White          13\n",
              "Yellow         14\n",
              "Name: col_codes, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_key=pattern_key.reset_index()\n",
        "category_key=category_key.reset_index()\n",
        "color_key=color_key.reset_index()"
      ],
      "metadata": {
        "id": "SP1xFyldgN9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save keys for use in other notebook to translate prediction back to text\n",
        "pattern_key.to_csv('/content/drive/My Drive/fashion_annotation/pattern_key.csv')\n",
        "category_key.to_csv('/content/drive/My Drive/fashion_annotation/category_key.csv')\n",
        "color_key.to_csv('/content/drive/My Drive/fashion_annotation/color_key.csv')"
      ],
      "metadata": {
        "id": "0wtCWYVDd8f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data into 6 parts to train separately\n",
        "#stratify based on category label\n",
        "\n",
        "X=data['url']\n",
        "y=data[['cat_codes','pat_codes','col_codes']]\n",
        "\n",
        "test_sizes=[.83,.8,.75,.67,.5]\n",
        "\n",
        "splits=[]\n",
        "\n",
        "for ts in test_sizes:\n",
        "  X_use, X, y_use, y = train_test_split(X,y,test_size=ts,\n",
        "                                        random_state=42,\n",
        "                                        stratify=y['cat_codes'])\n",
        "  split=pd.concat([X_use,y_use],axis=1)\n",
        "  print(split.shape)\n",
        "  splits.append(split)\n",
        "\n",
        "last_split=pd.concat([X,y],axis=1)\n",
        "print(last_split.shape)\n",
        "splits.append(last_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSuslp5wOQY",
        "outputId": "319b5d4f-81da-44c8-d628-593d17f4fe39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(68323, 4)\n",
            "(66715, 4)\n",
            "(66716, 4)\n",
            "(66048, 4)\n",
            "(67050, 4)\n",
            "(67050, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_0, val_0=folds[0]"
      ],
      "metadata": {
        "id": "CoaMEtL-25CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check distribution of classes across labels in all splits\n",
        "\n",
        "#number of classes per label\n",
        "\n",
        "class_counts=[]\n",
        "\n",
        "for split in splits:\n",
        "  class_num={}\n",
        "  class_num['train_cat']=split['cat_codes'].nunique()\n",
        "  class_num['train_pat']=split['pat_codes'].nunique()\n",
        "  class_num['train_col']=split['col_codes'].nunique()\n",
        "  class_counts.append(class_num)\n",
        "\n",
        "class_counts_df=pd.DataFrame(class_counts)\n",
        "class_counts_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "pJmsIO_R3CcU",
        "outputId": "bca3194d-cda7-443d-bfd6-fc839963db3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_cat  train_pat  train_col\n",
              "0         57         23         15\n",
              "1         57         23         15\n",
              "2         57         23         15\n",
              "3         57         23         15\n",
              "4         57         23         15\n",
              "5         57         23         15"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2218379-6fe5-4871-a1e4-6b868c37d119\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_cat</th>\n",
              "      <th>train_pat</th>\n",
              "      <th>train_col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2218379-6fe5-4871-a1e4-6b868c37d119')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2218379-6fe5-4871-a1e4-6b868c37d119 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2218379-6fe5-4871-a1e4-6b868c37d119');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create train and val sets in each split and verify stratification of labels\n",
        "train_val_splits=[]\n",
        "class_counts=[]\n",
        "\n",
        "for split in splits:\n",
        "  X=split['url']\n",
        "  y=split[['cat_codes','pat_codes','col_codes']]\n",
        "  X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=.2,\n",
        "                                               stratify=y['cat_codes'])\n",
        "  train_data=pd.concat([X_train,y_train],axis=1)\n",
        "  val_data=pd.concat([X_val,y_val],axis=1)\n",
        "  train_val_splits.append((train_data,val_data))\n",
        "\n",
        "  class_num={}\n",
        "  class_num['train_cat']=train_data['cat_codes'].nunique()\n",
        "  class_num['val_cat']=val_data['cat_codes'].nunique()\n",
        "  class_num['train_pat']=train_data['pat_codes'].nunique()\n",
        "  class_num['val_pat']=val_data['pat_codes'].nunique()\n",
        "  class_num['train_col']=train_data['col_codes'].nunique()\n",
        "  class_num['val_col']=val_data['col_codes'].nunique()\n",
        "  class_counts.append(class_num)\n",
        "\n",
        "counts_df=pd.DataFrame(class_counts)\n",
        "counts_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "RhrRWxfi1Sej",
        "outputId": "a7a35998-040f-43ff-bb1b-82d4b015f52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_cat  val_cat  train_pat  val_pat  train_col  val_col\n",
              "0         57       57         23       23         15       15\n",
              "1         57       57         23       23         15       15\n",
              "2         57       57         23       23         15       15\n",
              "3         57       57         23       23         15       15\n",
              "4         57       57         23       23         15       15\n",
              "5         57       57         23       23         15       15"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a515d3f7-8a28-4f64-893b-4f0070d77ce7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_cat</th>\n",
              "      <th>val_cat</th>\n",
              "      <th>train_pat</th>\n",
              "      <th>val_pat</th>\n",
              "      <th>train_col</th>\n",
              "      <th>val_col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a515d3f7-8a28-4f64-893b-4f0070d77ce7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a515d3f7-8a28-4f64-893b-4f0070d77ce7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a515d3f7-8a28-4f64-893b-4f0070d77ce7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save each train and val df as csv for reproducability\n",
        "\n",
        "for n in range(6):\n",
        "  train=train_val_splits[n][0]\n",
        "  val=train_val_splits[n][1]\n",
        "  t_path='/content/drive/My Drive/fashion_annotation/data_splits/train_'+str(n)+'.csv'\n",
        "  v_path='/content/drive/My Drive/fashion_annotation/data_splits/val_'+str(n)+'.csv'\n",
        "  train.to_csv(t_path)\n",
        "  val.to_csv(v_path)"
      ],
      "metadata": {
        "id": "touFael4HoOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Images"
      ],
      "metadata": {
        "id": "Z4ezBYEMJM4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, images are downloaded in to google drive. Each image is also tested to ensure it opens properly prior to training the model, since some of the urls did not contain good data and it stops the whole training process."
      ],
      "metadata": {
        "id": "yZq05QKd7-h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download images into the right folder on google drive"
      ],
      "metadata": {
        "id": "aEC4S2OKKCRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that takes url, downloads image and saves it to drive\n",
        "def write_im_data(url,path):\n",
        "\n",
        "  response=requests.get(url,stream=True)\n",
        "  im=response.content\n",
        "\n",
        "  with open (path+url[-30:]+'.bin','wb') as f:\n",
        "    f.write(im)"
      ],
      "metadata": {
        "id": "B7QHmlXMWqy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#takes a list of files and calls write_im_data to download each one\n",
        "def write_images(df,path):\n",
        "  count=0\n",
        "  urls=df['url'].to_list()\n",
        "  print('this many files: {}'.format(len(urls)))\n",
        "  for url in urls:\n",
        "    write_im_data(url,path)\n",
        "    count+=1\n",
        "    if count%1000==0:\n",
        "      print(count)\n",
        "  print('downloaded all files')"
      ],
      "metadata": {
        "id": "rkAcMr94X-Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/My Drive/fashion_annotation/imgs_split/train_2/'\n",
        "train_2=pd.read_csv('/content/drive/My Drive/fashion_annotation/data_splits/train_2.csv')\n",
        "\n",
        "write_images(train_2,path)"
      ],
      "metadata": {
        "id": "t9ExXIEUdW-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test if the urls in train_data and val_data are present in directory\n",
        "\n",
        "def find_missing_urls(urls):\n",
        "  bad_urls=[] \n",
        "\n",
        "  for url in urls:\n",
        "    try:\n",
        "      with open(path+url[-30:]+'.bin','rb') as f:\n",
        "        im=f.read()\n",
        "    except Exception as e:\n",
        "      bad_urls.append(url)\n",
        "    \n",
        "  return bad_urls\n",
        "\n",
        "#test if images in directory open properly\n",
        "def test_urls(df,path):\n",
        "  count=0\n",
        "  bad_urls=[] \n",
        "  urls=df['url'].to_list()\n",
        "  print('test {} images'.format(len(urls)))\n",
        "  for url in urls:\n",
        "    try:\n",
        "      with open(path+url[-30:]+'.bin','rb') as f:\n",
        "        im=f.read()\n",
        "        byte_im=io.BytesIO(im)\n",
        "        #byte_im.seek(0)\n",
        "        img=Image.open(byte_im)\n",
        "    except Exception as e:\n",
        "      bad_urls.append(url)\n",
        "    count+=1\n",
        "    if count%5000==0:\n",
        "      print(count)\n",
        "  print('tested all urls')\n",
        "  return bad_urls"
      ],
      "metadata": {
        "id": "Gq5QA0Zwc2Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/My Drive/fashion_annotation/imgs_split/train_1/'\n",
        "train_1=pd.read_csv('/content/drive/My Drive/fashion_annotation/data_splits/train_1.csv')\n",
        "\n",
        "bad_urls= test_urls(train_1,path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBw4SOZ_mWDg",
        "outputId": "60e8e218-08d7-4a02-e846-abd882fd9e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 53372 images\n",
            "5000\n",
            "10000\n",
            "15000\n",
            "20000\n",
            "25000\n",
            "30000\n",
            "35000\n",
            "40000\n",
            "45000\n",
            "50000\n",
            "tested all urls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OeHkjYESou9",
        "outputId": "1eedd3a4-cc3b-4105-a153-684bda138707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://contestimg.wish.com/api/webimage/565a6b3be389f71618d11c15-large',\n",
              " 'http://contestimg.wish.com/api/webimage/52376c551c238844b7acef99-large',\n",
              " 'http://contestimg.wish.com/api/webimage/54aeebf028565a09587cb876-large',\n",
              " 'http://contestimg.wish.com/api/webimage/56c2700fe7acaa15604a00e1-large',\n",
              " 'http://contestimg.wish.com/api/webimage/571df4756c085a5cf37ae849-large',\n",
              " 'http://contestimg.wish.com/api/webimage/54509e2de03c9e105de8d78a-large',\n",
              " 'http://contestimg.wish.com/api/webimage/53e06d0ad911394469ed784e-large',\n",
              " 'http://contestimg.wish.com/api/webimage/58df178a52c93869e1d07f08-large']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove bad urls\n",
        "\n",
        "def remove_urls(df,bad_urls,path):\n",
        "  print(df.shape)\n",
        "  for url in bad_urls:\n",
        "    df=df[df['url']!=url]\n",
        "  print(df.shape)\n",
        "  df.to_csv(path)\n",
        "  print('wrote new file')"
      ],
      "metadata": {
        "id": "GvtMfzGu04l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/My Drive/fashion_annotation/data_splits/train_1.csv'\n",
        "\n",
        "remove_urls(train_1,bad_urls,path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mlTdJtpTQeI",
        "outputId": "49857b5d-997d-4502-c78b-a62765dfa5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(53372, 5)\n",
            "(53364, 5)\n",
            "wrote new file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Prepared Data"
      ],
      "metadata": {
        "id": "AnLG3QxHGtS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the split to be trained on the model"
      ],
      "metadata": {
        "id": "Fm2xDwoh8cPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('/content/drive/My Drive/fashion_annotation/data_splits/train_1.csv')\n",
        "val_data=pd.read_csv('/content/drive/My Drive/fashion_annotation/data_splits/val_1.csv')"
      ],
      "metadata": {
        "id": "JdT6hFWjXL8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_num=train_data['cat_codes'].nunique()\n",
        "pat_num=train_data['pat_codes'].nunique()\n",
        "col_num=train_data['col_codes'].nunique()\n",
        "print(cat_num)\n",
        "print(pat_num)\n",
        "print(col_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUSpoIedu9iT",
        "outputId": "84d5c919-a889-48f6-d960-80b9e6d68046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57\n",
            "23\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_col_dict={'pattern':'pat_codes','category':'cat_codes','color':'col_codes'}"
      ],
      "metadata": {
        "id": "6r8HVTX2Mb9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Custom Image Generator"
      ],
      "metadata": {
        "id": "LS_A28WO_r3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a custom data generator that downloads images for a batch, resize, convert to numpy array.  "
      ],
      "metadata": {
        "id": "20mX68RXCRJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageGen(Sequence):\n",
        "  def __init__(self, df, X_col, y_col,\n",
        "               batch_size,\n",
        "               path,\n",
        "               input_size=(224,224,3),\n",
        "               shuffle=True):\n",
        "    self.df = df\n",
        "    self.X_col = X_col\n",
        "    self.y_col = y_col\n",
        "    self.batch_size = batch_size\n",
        "    self.input_size = input_size\n",
        "    self.shuffle = shuffle\n",
        "    self.path = path \n",
        "\n",
        "    self.n = len(self.df)\n",
        "    self.n_cat = df[y_col['category']].nunique()\n",
        "    self.n_pat = df[y_col['pattern']].nunique()\n",
        "    self.n_col = df[y_col['color']].nunique()\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    if self.shuffle:\n",
        "      self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  def __getim_(self,url):\n",
        "    \"\"\"\n",
        "    download image from url, open from byte string format,\n",
        "    resize and convert to np array (224,224,3)\n",
        "    \"\"\"\n",
        "    #print(url)\n",
        "    #response=requests.get(url,stream=True)\n",
        "    #im=response.content\n",
        "    with open(self.path+url[-30:]+'.bin','rb') as f:\n",
        "      im=f.read()\n",
        "    byte_im=io.BytesIO(im)\n",
        "    #byte_im.seek(0)\n",
        "    img=Image.open(byte_im)\n",
        "    img_sized=img.resize((224,224))\n",
        "    im_array=np.asarray(img_sized)\n",
        "    return im_array\n",
        "\n",
        "  def __get_images(self,url_batch):\n",
        "    \"\"\"\n",
        "    initiate image array for batch, \n",
        "    call __getim_ to load each image to array\n",
        "    preprocess batch for VGG and return array size (batch_size,224,224,3)\n",
        "    \"\"\"\n",
        "    dims=[x for x in self.input_size]\n",
        "    im_array=np.zeros((self.batch_size,dims[0],dims[1],dims[2]))\n",
        "\n",
        "    for count,url in enumerate(url_batch):\n",
        "      im_array[count,:,:,:]=self.__getim_(url)\n",
        "\n",
        "    #preprocess for VGG and return\n",
        "    return preprocess_input(im_array)\n",
        "\n",
        "  def __get_data(self, batches):\n",
        "    url_batch = batches[self.X_col]\n",
        "\n",
        "    cat_batch = batches[self.y_col['category']]\n",
        "    pat_batch = batches[self.y_col['pattern']]\n",
        "    col_batch = batches[self.y_col['color']]\n",
        "\n",
        "    X_array = self.__get_images(url_batch)\n",
        "\n",
        "    #ohe the output labels\n",
        "    y_cat=to_categorical(cat_batch,self.n_cat)\n",
        "    y_pat=to_categorical(pat_batch,self.n_pat)\n",
        "    y_color=to_categorical(col_batch,self.n_col)\n",
        "\n",
        "    return X_array, (y_cat,y_pat,y_color)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    batches=self.df[index*self.batch_size:(index+1)*self.batch_size]\n",
        "    X, y = self.__get_data(batches)\n",
        "    return X, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n // self.batch_size\n"
      ],
      "metadata": {
        "id": "GXvcIjzw_vup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "4IMMGFbLE1A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download VGG layers\n",
        "vgg=VGG16(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n",
        "vgg.trainable=False"
      ],
      "metadata": {
        "id": "3Ec5yVppERB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9c9127-69fa-4470-ff46-27276d0c748e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "58900480/58889256 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=Input(shape=IMAGE_SIZE)\n",
        "\n",
        "x = vgg(inputs, training=False)\n",
        "\n",
        "#Convert features of output shape to vectors\n",
        "branch = GlobalAveragePooling2D()(x)\n",
        "\n",
        "#Category Prediction Branch\n",
        "cat_branch1 = Dense(512, activation='relu')(branch)\n",
        "cat_branch2 = Dense(256, activation='relu')(cat_branch1)\n",
        "cat_pred=Dense(cat_num, activation='softmax')(cat_branch2)\n",
        "\n",
        "#Pattern Prediction Branch\n",
        "\n",
        "pat_branch1 = Dense(512, activation='relu')(branch)\n",
        "pat_branch2 = Dense(256, activation='relu')(pat_branch1)\n",
        "pat_pred=Dense(pat_num, activation='softmax')(pat_branch2)\n",
        "\n",
        "#Color Prediction Branch\n",
        "col_branch1 = Dense(512, activation='relu')(branch)\n",
        "col_branch2 = Dense(256, activation = 'relu')(col_branch1)\n",
        "col_pred=Dense(col_num, activation = 'softmax')(col_branch2)\n",
        "\n",
        "model=Model(inputs,outputs=[cat_pred,pat_pred,col_pred])"
      ],
      "metadata": {
        "id": "Cy04crnyI2Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzNMXwv1eL94",
        "outputId": "aa0aca8b-732e-4439-fa6b-591cc6df6056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " vgg16 (Functional)             (None, 7, 7, 512)    14714688    ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 512)         0           ['vgg16[0][0]']                  \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          262656      ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 512)          262656      ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 512)          262656      ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          131328      ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 256)          131328      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 256)          131328      ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 57)           14649       ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 23)           5911        ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 15)           3855        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,921,055\n",
            "Trainable params: 1,206,367\n",
            "Non-trainable params: 14,714,688\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining losses for each output\n",
        "losses ={ 'dense_2':CategoricalCrossentropy(),\n",
        "    'dense_5':CategoricalCrossentropy(),\n",
        "    'dense_8':CategoricalCrossentropy()}\n",
        "\n",
        "model.compile(\n",
        "    loss=losses,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "vvqSRPgWeTQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are continuing training on a previously trained model, it will be loaded in the cell below."
      ],
      "metadata": {
        "id": "v2WY5Avp9ECH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model=pickle.load(open('/content/drive/My Drive/fashion_annotation/model_3.pickle', 'rb'))\n"
      ],
      "metadata": {
        "id": "XsiGNLrlfJTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path='/content/drive/My Drive/fashion_annotation/imgs_split/train_1/'\n",
        "traingen = CustomImageGen(\n",
        "    train_data, 'url', y_col_dict, batch_size=32, path=train_path\n",
        ")\n",
        "\n",
        "val_path='/content/drive/My Drive/fashion_annotation/imgs_split/val_1/'\n",
        "valgen = CustomImageGen(\n",
        "    val_data, 'url', y_col_dict, batch_size=32, path=val_path\n",
        ")\n",
        "\n",
        "\n",
        "results = model.fit(traingen,\n",
        "                    validation_data=valgen,\n",
        "                    epochs=2)"
      ],
      "metadata": {
        "id": "zozBp2dBg7ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbfd89f-74e8-4530-eafa-a257757578ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1667/1667 [==============================] - 30765s 18s/step - loss: 4.3059 - dense_2_loss: 1.8730 - dense_5_loss: 0.9311 - dense_8_loss: 1.5017 - dense_2_accuracy: 0.4641 - dense_5_accuracy: 0.7522 - dense_8_accuracy: 0.5292 - val_loss: 4.2630 - val_dense_2_loss: 1.8238 - val_dense_5_loss: 0.9158 - val_dense_8_loss: 1.5235 - val_dense_2_accuracy: 0.4720 - val_dense_5_accuracy: 0.7564 - val_dense_8_accuracy: 0.5252\n",
            "Epoch 2/2\n",
            "1667/1667 [==============================] - 899s 539ms/step - loss: 4.0218 - dense_2_loss: 1.7164 - dense_5_loss: 0.8677 - dense_8_loss: 1.4377 - dense_2_accuracy: 0.4959 - dense_5_accuracy: 0.7605 - dense_8_accuracy: 0.5474 - val_loss: 4.2612 - val_dense_2_loss: 1.8209 - val_dense_5_loss: 0.9280 - val_dense_8_loss: 1.5123 - val_dense_2_accuracy: 0.4804 - val_dense_5_accuracy: 0.7486 - val_dense_8_accuracy: 0.5332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model and results\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/fashion_annotation/model_4.pickle', 'wb') as f:\n",
        "  pickle.dump(model, f)\n",
        "\n",
        "with open('/content/drive/My Drive/fashion_annotation/results_mod4.pickle', 'wb') as f:\n",
        "  pickle.dump(results, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4C3--2ZuS6W",
        "outputId": "0b2fd664-489b-41d1-816f-885505d2c5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://754d79d1-1aff-4576-bdc9-a063aff092e5/assets\n",
            "INFO:tensorflow:Assets written to: ram://7fe0dd8d-3623-42f7-a1d3-93f3ee48214f/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model\n",
        "\n",
        "The main goal of this project is not to produce the most accurate model possible. But it is still important to take a look at the results and understand how it is performing.  The accuracy scores are not particularly high (although much better than random guessing) and looking at confusion matrices can help us understand what may be causing the errors.\n",
        "\n",
        "Below is the accuracy score for each attribute:\n",
        "\n",
        "<ol>\n",
        "<li>Category: 48%</li>\n",
        "<li>Pattern: 75%</li>\n",
        "<li>Color: 53%</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "FgTtAG3iBID7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model=pickle.load(open('/content/drive/My Drive/fashion_annotation/model_4.pickle', 'rb'))\n"
      ],
      "metadata": {
        "id": "0xCH0wp2MxYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_path='/content/drive/My Drive/fashion_annotation/imgs_split/val_1/'\n",
        "valgen = CustomImageGen(\n",
        "    val_data, 'url', y_col_dict, batch_size=32, path=val_path\n",
        ")"
      ],
      "metadata": {
        "id": "oQiGwsuMM3Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model.predict(x=valgen)"
      ],
      "metadata": {
        "id": "EGWJkdhDM5YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pred=np.argmax(prediction[0],axis=1)\n",
        "pat_pred=np.argmax(prediction[1],axis=1)\n",
        "col_pred=np.argmax(prediction[2],axis=1)"
      ],
      "metadata": {
        "id": "9V85Wu6CCr6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwDBy2vrSeP2",
        "outputId": "ac999120-61bf-45c9-ef59-1a08b4d1d74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([45, 26, 43, ...,  9,  4, 41])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/fashion_annotation/cat_pred.pickle', 'wb') as f:\n",
        "  pickle.dump(cat_pred, f)\n",
        "\n",
        "with open('/content/drive/My Drive/fashion_annotation/pat_pred.pickle', 'wb') as f:\n",
        "  pickle.dump(pat_pred, f)\n",
        "\n",
        "with open('/content/drive/My Drive/fashion_annotation/col_pred.pickle', 'wb') as f:\n",
        "  pickle.dump(col_pred, f)"
      ],
      "metadata": {
        "id": "A0F_QvXzSfMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "cat_pred=pickle.load(open('/content/drive/My Drive/fashion_annotation/cat_pred.pickle', 'rb'))\n",
        "pat_pred=pickle.load(open('/content/drive/My Drive/fashion_annotation/pat_pred.pickle', 'rb'))\n",
        "col_pred=pickle.load(open('/content/drive/My Drive/fashion_annotation/col_pred.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "ud4HrSgbS6VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fv_YScW3wS5",
        "outputId": "d13299e7-a05d-4cd9-91cc-57a249d0a5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13312,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_true=val_data['cat_codes'][:13312]\n",
        "pat_true=val_data['pat_codes'][:13312]\n",
        "col_true=val_data['col_codes'][:13312]"
      ],
      "metadata": {
        "id": "ddwoSuOy3zwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Category Confusion Matrix\n",
        "\n",
        "We can see from the confusion matrix that there are errors distributed across most of the categories but 19 in particular seems to be error prone. This is the Dress Shirts category. Perhaps this is a case where the definition of what constitutes a dress shirt was not strict enough in the labeling process and it is too difficult for the model to distinguish the differences."
      ],
      "metadata": {
        "id": "KBmbarVQCjvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(confusion_matrix(cat_true,cat_pred),cmap=\"YlGnBu\",vmax=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "6zLju6NA5c_V",
        "outputId": "d212ab93-605b-4edc-9247-72019a83c3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f29a23438d0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pattern Confusion Matrix\n",
        "\n",
        "For this attribute category 10 is extremely problematic. Most of the other categories perform well. This corresponds to 'None' indicating a solid color.  This is not surprising and is likely due to confusing labeling in the other categories. In the data cleaning all images with null values were assigned 'None' but some of these could have also been missing data instead of an indication that there was no pattern present. It should also be noted that although most of the errors are concentrated in this category, it is also what the vast majority of the images were classified as."
      ],
      "metadata": {
        "id": "ubSPNwsXDvJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn\n",
        "seaborn.heatmap(confusion_matrix(pat_true,pat_pred),vmax=50,cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "97Kuoi3f5Cwc",
        "outputId": "40d924d8-68e4-4869-956a-e3ef374e2fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f29a243fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Color Confusion Matrix\n",
        "\n",
        "Errors for color are distributed across categories although 1, 11, and 13 seem to be particularly problematic.  This corresponds to Black, Red, and White. To increase accuracy, one would need to more closely examine labeling for these images."
      ],
      "metadata": {
        "id": "UsGrejHRETcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.heatmap(confusion_matrix(col_true,col_pred),cmap=\"YlGnBu\",vmax=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "y4wtEF7j6BlB",
        "outputId": "754490c4-aa7b-43bc-f556-3895c35186ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f29a2255950>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Pipeline"
      ],
      "metadata": {
        "id": "5QnbpXix8GXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model has been trained, the pipeline has to be developed to retrieve Instagram images, assign annotations, and store them in a database to be used for a Google Data Studio report.  This process is outlined in the second notebook."
      ],
      "metadata": {
        "id": "x53Q451RFuUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions"
      ],
      "metadata": {
        "id": "sb0GFtIZGWvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is a proof of concept that demonstrates how to provide more detailed Instagram fashion trend data to end users using a neural network that can assign three different attributes to pictured clothing. \n",
        "\n",
        "The model used is 48% accurate for category of clothing, 75% accurate for pattern, and 53% accurate for color. Due to resource and time constraints the model could not be developed further, but there are many avenues available to improve the outcomes and it already shows a much higher ability to assign annotations that random guessing among the options.\n",
        "\n",
        "The report was produced using 6 hashtags popular among fashion influencers to retrieve fashion images from Instagram posts. However, to have a broad overview of fashion trend data it would be important to carefully select hashtags, consider targeting certain user accounts (known influencers or set threshold for minimum number of followers), and ensure a sufficient quantity is collected.  These adjustments could easily be made in the second notebook for the prediction pipeline after sufficient research was done."
      ],
      "metadata": {
        "id": "nbrS6iUXGZ6t"
      }
    }
  ]
}